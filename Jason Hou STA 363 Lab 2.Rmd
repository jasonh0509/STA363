---
title: "Jason Hou STA 363 Lab 2"
output: html_document
date: '2022-09-01'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r load package, include=FALSE}
library(tidyverse)
library(broom)
library(janitor)
library(ggplot2)
library(class)
suppressMessages(library(class))
library(gridExtra)
```


```{r load data}
training<-read.csv("training.csv")
testing<-read.csv("testing.csv")
```

```{r include=FALSE}
glimpse(training)
```

## Question 1

*How many rows are in the test data set? What about the training data set?*

```{r}
nrow(training)
```

```{r}
nrow(testing)
```

There are 900 rows in the test data set and 3012 rows in the testing data set.

## Question 2

*The test data set is smaller than the training data set. This is almost always the case in statistical learning applications. Explain why you think this is the case.*

The test data set is smaller than the training data set because we would need more data points to train the model to obtain a better model for better prediction/association.

## Question 3

*(a)Create a scatter plot where line breaks is on the x axis and the number of characters is on the Y axis.* 

*(b) Did you use your training or test data set to create your plot?*

```{r scatterplot}
Train_Scatter<-training%>%
  ggplot(mapping = aes(x=line_breaks,y=num_char))+
  geom_point(color="blue")+
  ggtitle("Figure 1: Scatterplot of Numer Line Breaks vs Number of Characteris in Email")
Train_Scatter
```

I used training data set to create the plot.

## Question 4

*Change the color and/or the shape of the points on the graph based on Y. In other words, spam emails should be one shape and/or color and non spam emails should be a different shape and/or color. Hint: Look back at Lab 1 if you have questions on how to do this!*

```{r change color shape}
Train_Scatter_withColor<-training%>%
  ggplot(mapping = aes(x=line_breaks,y=num_char,color=spam,pch=spam))+
  geom_point()+
  ggtitle("Figure 2: Scatterplot of Numer Line Breaks vs Number of Characteris in Email
          (by spam/not spam)")
Train_Scatter_withColor
```

## Question 5

*Suppose an email has 175 characters and 3800 line breaks. Using the 3-nearest neighbors, what value $\hat{Y}$ would you predict for this email? Hint: You donâ€™t need any code for this; look at the graph from Question 4.*

Based on the graph from Question 4, we can locate this email near the top right corner of the graph, and I will predict as not spam based on the status of the nearest three neighboring data points. 

## Question 6

*Using 5 nearest neighbors, run KNN. Look at the object results you have created. Is this a scalar (one value), a vector (one column or one row), or a matrix (multiple rows and columns)?*

```{r knn, include=FALSE}
results <- knn(train = training[,c(12,13)] , test=testing[,c(12,13)], cl = training$spam, k = 5 )
results
```

The object "result" is a vector.

```{r knn table}
table1<-table(results,testing$spam)
table1
```


## Question 7

*Make a table to show how many emails 5-nearest neighbors predicted to be spam and not spam.*

```{r table confusion Matrix}
knitr::kable(table(results, testing$spam), caption= "Table 1", col = c("True Not Spam", "True Spam"))
```

## Question 8

*What is the sensitivity of your classification method? Do we prefer methods with larger or smaller sensitivity?*

$\frac{17}{75+17}\approx0.185$

The sensitivity of the classification method is 0.185. We prefer methods to have a higher sensitivity.

## Question 9

*What is the specificity of your classification of your classification method? Do we prefer methods with larger or smaller specificity?*

$\frac{791}{791+17}\approx0.979$

The specificity of the classification method is about 0.979. We prefer a method with higher specificity.


## Question 10

*What is the false positive predictive rate (FPPR)? Do we prefer methods with larger or smaller FPPR?*

$\frac{17}{17+17}=0.5$

## Question 11

*What is the false negative predictive rate (FNPR)? Do we prefer methods with larger or smaller FNPR?*


$\frac{75}{791+75}\approx0.087$


## Question 12

*What is the accuracy of your classification approach? Hint: This the proportion of the predictions that are correct.*

$\frac{791+17}{900}\approx0.898$


## Question 13

*Run kNN using k=3,7,9. State the sensitivity and specificity that you get using each choice if k.*

```{r knn 3}
knn3<-knn(train = training[,c(12,13)] , test=testing[,c(12,13)], cl = training$spam, k = 3 )

knitr::kable(table(knn3, testing$spam), caption= "Table 2: Confusion Matrix of Spam/Not Spam Emails Prediction:(k=3) ", col = c("True Not Spam", "True Spam"))
```

$Sensitivity=\frac{24}{68+24}\approx0.261$

$Specificity=\frac{779}{779+29}\approx0.964$

$Accuracy=\frac{779+24}{900}\approx0.892$

```{r knn7}
knn7<-knn(train = training[,c(12,13)] , test=testing[,c(12,13)], cl = training$spam, k = 7 )

knitr::kable(table(knn7, testing$spam), caption= "Table 3: Confusion Matrix of Spam/Not Spam Emails Prediction:(k=7)", col = c("True Not Spam", "True Spam"))
```

$Sensitivity=\frac{17}{75+17}\approx0.185$

$Specificity=\frac{799}{799+9}\approx0.989$

```{r knn9}
knn9<-knn(train = training[,c(12,13)] , test=testing[,c(12,13)], cl = training$spam, k = 9 )

knitr::kable(table(knn9, testing$spam), caption= "Table 4: Confusion Matrix of Spam/Not Spam Emails Prediction:(k=9)", col = c("True Not Spam", "True Spam"))
```

$Sensitivity=\frac{17}{17+75}\approx0.185=18.5%$

$Specificity=\frac{797}{797+11}\approx0.988=98.8%$


## Question 14

*What trend to you notice in the specificity as the k increases? Explain why you think this is happening as k increases.*

We noticed that as the k increase, the specificity increases from 96.4% to 98.9% when k increases from 3 to 7 but decreased slightly when k increases from 7 to 9 (from 98.9% to 98.8%, in other words, specificity hit the plateau when k reaches 9).

We believe this phenomenon happened because as k increases, there are more points included in the nearest-neighbor range, which increased the error of prediction, in other words, increased the chance of having spam emails labeled as non_spam emails. 


## Question 15

*Looking at the sensitivity and specificity from k=3,5,7,9, which k would you choose? Explain your choice.*

I will choose k=3 since it has the highest sensitivity, in other words, the highest true positive rate of the prediction, among all four choices of k.

## Question 16


```{r function empirical logit, include=FALSE}
emplogitPlot <- function(x, y, binsize = NULL, ci = FALSE, probit = FALSE,
prob = FALSE, main = NULL, xlab = "", ylab = "", lowess.in = FALSE){
  # x         vector with values of the independent variable
  # y         vector of binary responses
  # binsize   integer value specifying bin size (optional)
  # ci        logical value indicating whether to plot approximate
  #           confidence intervals (not supported as of 02/08/2015)
  # probit    logical value indicating whether to plot probits instead
  #           of logits
  # prob      logical value indicating whether to plot probabilities
  #           without transforming
  #
  # the rest are the familiar plotting options
  
  if(class(y) =="character"){
   y <- as.numeric(as.factor(y))-1
   }
  
  if (length(x) != length(y))
    stop("x and y lengths differ")
  if (any(y < 0 | y > 1))
    stop("y not between 0 and 1")
  if (length(x) < 100 & is.null(binsize))
    stop("Less than 100 observations: specify binsize manually")
  
  if (is.null(binsize)) binsize = min(round(length(x)/10), 50)
  
  if (probit){
    link = qnorm
    if (is.null(main)) main = "Empirical probits"
  } else {
    link = function(x) log(x/(1-x))
    if (is.null(main)) main = "Empirical logits"
  }
  
  sort = order(x)
  x = x[sort]
  y = y[sort]
  a = seq(1, length(x), by=binsize)
  b = c(a[-1] - 1, length(x))
  
  prob = xmean = ns = rep(0, length(a)) # ns is for CIs
  for (i in 1:length(a)){
    range = (a[i]):(b[i])
    prob[i] = mean(y[range])
    xmean[i] = mean(x[range])
    ns[i] = b[i] - a[i] + 1 # for CI 
  }
  
  extreme = (prob == 1 | prob == 0)
  prob[prob == 0] = min(prob[!extreme])
  prob[prob == 1] = max(prob[!extreme])
  
  g = link(prob) # logits (or probits if probit == TRUE)
  
  linear.fit = lm(g[!extreme] ~ xmean[!extreme])
  b0 = linear.fit$coef[1]
  b1 = linear.fit$coef[2]
  
  loess.fit = loess(g[!extreme] ~ xmean[!extreme])
  
  plot(xmean, g, main=main, xlab=xlab, ylab=ylab)
  abline(b0,b1)
  if(lowess.in ==TRUE){
  lines(loess.fit$x, loess.fit$fitted, lwd=2, lty=2)
  }
}
```


```{r empirical log odd plot 1}
emplogitPlot(x=training$line_breaks, y=training$spam, 
             xlab = "Line Breaks", 
             ylab = "Log Odds of Being Spam", 
             main = "Figure 3")
```

I don't feel comfortable claiming that the shape of the condition is satisfying, because the linear approach does not offer a good fit for the relationship between predictor Line Breaks and log odds of being spam email.


## Question 17

*Create an empirical logit plot where the log of line breaks is on the x axis.Which of the empirical logit plots (using line breaks or using log line breaks) is more linear? Based on the plots, choose whether you will use line breaks or log line breaks as a feature in the model.*


```{r transformation with log}
emplogitPlot(x=log(training$line_breaks), y=training$spam, 
             xlab = "Log of Line Breaks", 
             ylab = "Log Odds of Being Spam", 
             main = "Figure 4")
```

The empirical logit plots using log line breaks are more linear. Based on the plots, log line breaks will be used as a feature in the model.

## Question 18 
*Repeat this process for number of characters.*

```{r num_char logit}
emplogitPlot(x=training$num_char, y=training$spam,
             xlab = "Log of Number of Characters",
             ylab = "Log Odds of Being Spam",
             main = "Figure 4")
```

```{r num_char with log}
emplogitPlot(x=log(training$num_char), y=training$spam,
             xlab = "Log of Number of Characters",
             ylab = "Log Odds of Being Spam",
             main = "Figure 5")
```


```{r}
training<-training%>%
  mutate(spam=recode(spam,
                     "notspam" = "0",
                     "spam" = "1"))

training<-training%>%
  mutate(spam=as.numeric(spam))
```

## Question 19

*Train your chosen logistic regression model. Write down the trained logistic regression line in log odds form. Hint: For a template for write out your model in Markdown, copy and paste the following into the white space (NOT a chunk), and adapt it to match your trained model:*


```{r model 1, include=FALSE}
m1<-glm(formula=spam ~ log(line_breaks)+log(num_char),family = "binomial",data = training)
#summary(m1)$coefficient
```

```{r organized m1}
tidy(m1)
```


From the summary of model m1 we have:

$\beta_0=1.09$

$\beta_1=-0.87$

$\beta_2=0.18$


Thus,the trained model is: 

$$log\left(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\right) =1.09-0.87log(line\_breaks_i) + 0.18log(num\_char_i) $$
 
## Question 20

*Use your trained model to make predictions $\hat{Y}$ for each email in the test data set. Show a confusion matrix comparing the true values of $\hat{Y}$ in the test data to your predictions.*

```{r}
probabilities<-predict(m1,newdata = testing,type = "resp")
#probabilities
```

```{r confusion matrix for logistic}
predict.Y<-ifelse(probabilities>0.5,1,0)

knitr::kable(table(predict.Y, testing$spam), caption= "Table 4: Confusion Matrix of Spam/Not Spam Emails Prediction:(Logistic)", col = c("True Not Spam", "True Spam"))
```

## Question 21

*What is the sensitivity of your classification approach?*

The sensitivity of this classification approach is $Sensitivity=\frac{2}{90+2}\approx0.02=2\%$

## Question 22

*What is the specificity of your classification approach?*

The specificity of this classification approach is $Specificity=\frac{808}{808}\approx1=100\%$.

## Question 23

*What is the FPPR?*

The FPPR of this classification approach is $FPPR=\frac{0}{2}\approx0$

## Question 24

*What is the FNPR?*

The FNPR of this classification approach is  $FNPR=\frac{90}{808+90}\approx0.1=10\%$


## Question 25

*What is the accuracy of your classification approach?*


The accuracy of this classification approach is $\frac{2+808}{900}=0.9=90\%$.

## Question 26

*Compare the two classification approaches (kNN with your chosen k or logistic regression) in terms of (1) sensitivity, (2) specificity, and (3) accuracy.*

|             | KNN   | Logistic | Winner   |
|-------------|-------|----------|----------|
| Sensitivity | 0.261 | 0.02     | KNN      |
| Specificity | 0.964 | 1        | Logistic |
| Accuracy    | 0.892 | 0.9      | Logistic |

Based on the table, we could find that KNN provided a significantly better sensitivity compared to logistic regression. Logistic regression provided slightly better performance than KNN on specificity and accuracy in identifying spam emails. Therefore, using KNN with k=3 will be the appropriate choice for building the spam filter for the particular set of emails. 
