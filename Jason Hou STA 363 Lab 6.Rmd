---
title: "Jason Hou STA 363 Lab 6"
author: "Jason Hou"
date: '2022-10-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages, include=FALSE}
#Load all required packages
library(ggplot2)
library(broom)
library(gridExtra)
library(dplyr)
library(class)
library(tidyverse)
library(gridExtra)
library(leaps)
library(corrplot)
library(RColorBrewer)
library(glmnet)
```

```{r load data, include=FALSE}
#loading data needed
collegeData<-read.csv("collegedata.csv")
```

```{r take a look, include=FALSE}
glimpse(collegeData)
```


## Question 1
*Is this a regression or classification task? Based on this, what metric will we likely use to assess predictive accuracy?*

It is a regression task since the response variable is numeric. We are likely to use the MSE/RMSE to assess the predictive accuracy

## Question 2
*Make a visualization to explore the distribution of graduation rate. What is the smallest value of graduation rate? The largest?*

```{r distribution, warning=FALSE}
ggplot(data=collegeData,aes(x=Grad.Rate))+
  geom_boxplot()+
  ggtitle("Figure 1. Distribution of Graduation Rate")
```

```{r max and min graduation rate}
gradmax<-max(collegeData$Grad.Rate)
gradmin<-min(collegeData$Grad.Rate)
knitr::kable(c("Largest" = gradmax,"Smallest" = gradmin),caption =  "Table 1. Largest and Smallest")
```

The smallest value of graduation rate is 10. The largest value of graduation rate is 100.

## Question 3

*Compute the MSE (training) and RMSE (training) for these data using $\hat{Y}_i = \bar{Y}$ for all rows i.*

```{r MSE and RMSE}
## MSE = RSS/n
MSE<-sum((collegeData$Grad.Rate-mean(collegeData$Grad.Rate))^2)/776
RMSE<-sqrt(MSE)
knitr::kable(c("MSE" = MSE,"RMSE" = RMSE),caption = "Table 2.MSE and RMSE")
```

## Question 4
*Train an LSLR (OLS) regression model using the whole data set and the single feature X = the student faculty ratio. Find the MSE (training) and RMSE (training). How much has our training predictive accuracy improved from the values you got in Question 3?*

```{r faculty ratio only}
fac_ratio_only<-lm(data = collegeData, Grad.Rate~S.F.Ratio)
SFonly_Pred<-data.frame("Y_hat" = rep(NA,776))
SFonly_Pred$Y_hat<-predict(fac_ratio_only)
MSE_fac_Ratio<-sum((SFonly_Pred$Y_hat-mean(SFonly_Pred$Y_hat))^2)/776
RMSE_fac_Ratio<-sqrt(MSE_fac_Ratio)
```

$$MSE-MSE\_fac\_Ratio=291.51-27.80=263.71$$

$$RMSE-RMSE\_fac\_Ratio=17.07-5.27=11.8$$

The MSE has improved for 263.71. The RMSE has improved for 11.8.



## Question 5
*Which column in the data set cannot be used as a feature (aside from the response variable)? Explain why this column cannot be used.*

The column that can not be used is the X, which is the name of each university, because it has 776 different levels, plus, the name of school is useless when it comes to predict the graduate rate of the each institution.

##  Question 6
*Build a correlation plot (you can choose the styling you like best!) to explore the correlations in the features in this data set.*


```{r}
collegeData$Private<-ifelse(collegeData$Private == "Yes",1,0)
M<-cor(collegeData[,2:19])
corrplot(M, method = "circle",type = "upper",title = "Figure 2. Correlation Plot of Features in CollegeData",mar = c(0,0,1,0))
```

## Question 7
*Based on the plot in Question 6, why would you suggest we start with Ridge Regression instead of LSLR?*

I would suggest to start with Ridge Regression instead of LSLR since we have several columns that are highly correlated.

## Question 8

*Using the code above, create the design matrix. State the dimensions of this matrix.*

```{r desigend matrix}
XD <- model.matrix(Grad.Rate ~ ., data = collegeData[,-1])
```

## Question 9
*Suppose we let λ=2. Adapting the code above, what is β^Ridge?*

```{r}
# Train Ridge
ridge.model2<- glmnet(XD[,-1], collegeData$Grad.Rate, alpha = 0, lambda =2 , standardize = TRUE)
```

```{r lambda 2}
# Print the coefficients
Betas <- data.frame("Ridge" = as.numeric(coefficients(ridge.model2)))
rownames(Betas) <- colnames(XD)
knitr::kable(Betas,
            caption = "Table 3. Coefficients with Tuning Parameter = 2")
```


## Question 10
*What is the sum of the coefficients (excluding the intercept) when λ=2? Hint: To find this, use sum(ridge.model2$beta).*

```{r sum of coefficient}
knitr::kable(sum(ridge.model2$beta),caption = "Table 4. Sum of Coefficients when lambda = 2 ")
```

The sum of the coefficient(excluding the intercept) when $\lambda=2$ is 4.137.

## Question 11

*Suppose we let $\lambda=200$. Adapting the code above, what is $β^Ridge$?*

```{r}
ridge.model200<- glmnet(XD[,-1], collegeData$Grad.Rate, alpha = 0, lambda =200 , standardize = TRUE)


Betas200 <- data.frame("Ridge" = as.numeric(coefficients(ridge.model2)))
rownames(Betas200) <- colnames(XD)
knitr::kable(Betas200,
            caption = "Table 5. Coefficients with Tuning Parameter = 200 ")
```

## Question 12

*What is the sum of the coefficients (excluding the intercept) when λ=200?*

```{r sum lambda 200}
knitr::kable(sum(ridge.model200$beta),caption = "Table 6. Sum of Coefficients when lambda = 200")
```

The sum of the coefficients(excluding the intercept when $\lambda = 200$ is 0.873)

## Question 13
*What do you notice happens to the sum of the coefficients as λ increases?*

The sum of the coefficients decreases as $\lambda$ increases. 

## Question 14

*Briefly explain the process of choosing the tuning parameter in ridge regression. Use words, not code!*

To select the best tuning parameter in ridge regression, we have to choose the range of $\lambda$ that we want to consider first.Then, we utilize the build-in function cv.glmnet() to find out the best tuning parameter in ridge regression. 
Firstly, we have to set a random seed before using cv.glmnet(), then enter the design matrix(with unusable column removed), the response variable, value of alpha(here we use 0 because it instructs R that we are performing Ridge Regression),the sequence of $\lambda$ we would like to test for, and standardization indicator(set as TRUE). The cv.glmnet function will perform 10 fold CV for each value of $\lambda$ based on the features and response variable provided, and record the metrics that includes MSE and RMSE under each value of $\lambda$.Lastly, we choose the value of $\lambda$ that optimizes the test metric chosen(in this case, the MSE and RMSE).




## Question 15
*Why do we need to set a random seed here?*
```{r set seed}
set.seed(100)
ridge.mod <- cv.glmnet(XD[,-1], collegeData$Grad.Rate , alpha = 0 , lambda = seq(from=0, to=50,by=0.05) , standardize = TRUE)
```

Because the k-fold cross validation involves a random sampling process, setting a random seed will the replication of same outcome when the code was used across the devices.

## Question 16
*Why do we want to make sure to include λ=0 in our list of possible tuning parameters?*
Because we want to compare it with LSLR, while the result from $\lambda=0$ is the same as LSLR.

## Question 17

```{r load a function, include=FALSE}
ridgePlot <- function(ridge.mod, metric, title){
  library(ggplot2)
  
  smallestLambda <- ridge.mod$lambda[which.min(ridge.mod$cvm)] 
  
  if(metric == "MSE"){
  g1 <- ggplot( data.frame(ridge.mod$lambda), aes( x = ridge.mod$lambda, y = (ridge.mod$cvm))) + geom_point() + geom_vline( xintercept = smallestLambda, col = "blue" , lty = 2 ) +  labs(caption = paste("Test MSE values for Different Tuning Parameters. Smallest MSE at lambda = ", smallestLambda), title = title, y = "Test MSE", x = "Tuning Parameter")
  
  }
  
  if(metric == "RMSE"){
  g1 <- ggplot( data.frame(ridge.mod$lambda), aes( x = ridge.mod$lambda, y = sqrt(ridge.mod$cvm))) + geom_point() + geom_vline( xintercept = smallestLambda, col = "blue" , lty = 2 ) +  labs(caption = paste("Test RMSE values for Different Tuning Parameters. Smallest RMSE at lambda = ", smallestLambda), title = title, y = "Test RMSE", x = "Tuning Parameter")

  }
  
  g1
}
```


```{r lambda plot}
ridgePlot(ridge.mod, metric = "RMSE" , title ="Figure 3.RMSE in Response to Lambda" )

```

The $\lambda = 2.7$ gives the best value for our test metric(RMSE).

## Question 18
*Why might it be helpful to look at the plot instead of just using ridge.mod$lambda.min all the time?*

Because the plot shows the change of MSE as the lambda changes as a curve, which would enable us to find the ideal range of $\lambda$ that should be used in the code, as well as the best $\lambda$ value for the ridge regression. 

```{r lambda minimum}
minMSE<-min(ridge.mod$cvm)
knitr::kable(minMSE,caption = "Table 7. Minimum MSE")
```

The test MSE we would obtain using the ridge regression is 164.54.


## Question 19
*Using your choice of λ, what is the estimated test RMSE?*

```{r RMSE with chosen lambda}
knitr::kable(sqrt(minMSE),caption = "Table 8.  Estimated test MSE with Lambda = 2.7")
```

## Question 20
*How much has our RMSE improved from the values you got in Question 3? This allows us to see how much better our model does than using no feature information.*

```{r}
EstRMSE<-sqrt(minMSE)
improved<-RMSE-EstRMSE
knitr::kable(improved,caption = "Table 9. Improved Amount RMSE")
```

The RMSE improved 4.246 from the values we got in Question 3.

## Question 21
*Using the code above as a template, train your ridge regression model. Then, train the LSLR model using the same code template and call the result lslr.final. Once we have trained the model, we can make a data frame holding the coefficients for both LSLR and ridge using the following code. Show the resultant data frame as the answer to this question. Make sure you have formatted it using knitr::kable().*

```{r models}
ridge.final <- glmnet(XD[,-1], collegeData$Grad.Rate, alpha = 0 , lambda = 2.7 , standardize = TRUE)
lslr.final<- glmnet(XD[,-1], collegeData$Grad.Rate, alpha = 0 , lambda = 0 , standardize = TRUE)
ridge.betas <- as.numeric(coefficients(ridge.final))
lslr.betas <- as.numeric(coefficients(lslr.final))
BetasFinal <- data.frame("LSLR" = lslr.betas, "Ridge" = ridge.betas)
rownames(BetasFinal) <- colnames(XD)
knitr::kable(BetasFinal, caption = "Table 10. Coefficients for LSLR and Ridge")
```



## Question 22
*State the estimated test RMSE that you got from 10-fold CV for both of the two trained models (LSLR or ridge regression). Which has the best predictive accuracy? By how much (in percent difference)?NOTE: How do we do this in code???*

```{r}
# Run 10-fold CV for only the two choices of lambda
set.seed(100)
ridge.final.out <- cv.glmnet(XD[,-1], collegeData$Grad.Rate , alpha = 0 , lambda = c(0, 2.7), standardize = TRUE)
# Output the Lambdas
lambda_final<-ridge.final.out$lambda
# Output the test MSE
MSE_final<-ridge.final.out$cvm
# Output the test RMSE
RMSE_final<-sqrt(ridge.final.out$cvm)

LSLR_and_Ridge<-data.frame("Type" = c("Ridge","LSLR") ,"Lambda" = lambda_final, "MSE" = MSE_final, "RMSE" = RMSE_final)
knitr::kable(LSLR_and_Ridge,caption = "Table 11. Estimated MSE and RMSE for Ridge and LSLR")
```

$$Percent\ Difference\ between Ridge\ and\ LSLR = \frac{12.963-12.828}{12.963} \approx 0.0104$$

The estimated test RMSE got from 10-fold CV for LSLR is 12.963.The estimated test RMSE for ridge regression is 12.828. The ridge regression has the best predictive accuracy, it performed better than LSLR by approximately $1.04\%$. 
