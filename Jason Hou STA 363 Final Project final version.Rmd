---
title: "Jason Hou STA 363 Final Project"
author: "Jueshen Hou"
date: '2022-11-25'
output: html_document
---

```{r load packages, include=FALSE}
##Here we load the packages needed and install a package that is not on CRAN
library(ggplot2)
library(broom)
library(gridExtra)
library(dplyr)
library(class)
library(tidyverse)
library(gridExtra)
library(leaps)
library(corrplot)
library(RColorBrewer)
library(glmnet)
library(xtable)
library(randomForest)
library(pROC)
#library(smotefamily)
library(devtools)
devtools::install_github("dongyuanwu/RSBID")
library(RSBID)
library(UpSetR)
library(naniar)
library(report)
```

## Abstract

## Section 1: Data and Motivation

The data that we are dealing on is a data consist of 5110 rows and 12 columns.The data set record the 12 features of 5110 suspected stroke patients. We have the information on the patient's biological gender, the age of each patient in years,whether a patient has heart disease/hypertension, marital status, smoking status, stroke status and other related information. For other features not mentioned, please refer to the following link for a detailed description of features in this data set.(Fedesoriano) Link: <https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset>

Our goal is to build a classifier that helps to predict stroke event based on the health condition, health measures and related information provided. This classifier can be utilized to assist the diagnosis of stroke event along with other testing techniques.

## Section 2: Data Cleaning and EDA

```{r load data}
originalStroke<-read.csv("healthcare-dataset-stroke-data.csv")
```

```{r take a look}
glimpse(originalStroke)
```


```{r include=FALSE}
v1<-class(originalStroke$hypertension)
v2<-class(originalStroke$heart_disease)
v3<-class(originalStroke$stroke)
columns_to_convert<-c(v1,v2,v3)
features<-c("hypertension","heart_disease","stroke")
features_to_convert<-data.frame("Features" = features,"Type" = columns_to_convert)
```


We started with converting the data types of several features to more appropriate type. We found three features that need to be converted from numeric data to categorical data, which are hypertension, heart_disease, and stroke. These three features were converted from numeric type to categorical type. Other categorical features that are originally in character type were also converted into factor type for the model building later.

```{r change data type}
originalStroke$bmi<-as.numeric(originalStroke$bmi)
originalStroke$stroke<-as.factor(originalStroke$stroke)
originalStroke$hypertension<-as.factor(originalStroke$hypertension)
originalStroke$heart_disease<-as.factor(originalStroke$heart_disease)

#originalStroke$stroke<-as.factor(originalStroke$stroke)
```

```{r mutate variable, eval=FALSE, include=FALSE}
## mutate smoking status
originalStroke<-originalStroke%>%
    mutate(smoking_status=case_when(smoking_status=="formerly smoked"~"1",                        smoking_status=="smokes"~"1",  smoking_status=="never smoked"~"0",smoking_status=="Unknown"~"0"))
```

```{r change data type2}
#Change all categorical data that are in character type to factor type
originalStroke <- as.data.frame(unclass(originalStroke),stringsAsFactors=TRUE)
```


We then check for NA's in the data set. The result was shown in the table below.



```{r check NA after conversion}
# Check missing data
knitr::kable(sum(is.na(originalStroke))
,caption = "Table 2.1 Number of Missing Values")
```

```{r show where NA is}
NA_plot<-vis_miss(originalStroke)
NA_plot+
  ggtitle("Figure 2.1 Graph of NA in Each Column")
```

```{r bmi na row}
bmi_na_row<-which(is.na(originalStroke$bmi))
```

```{r missing value row stroke status}
knitr::kable(table(originalStroke$stroke[bmi_na_row]),caption = "Table 2.2 Stroke Status of Rows with Missing Value")
```

Table 2.1 and Figure 2.1 show that 201 rows of missing values were found from the data set, which were only present in the column "bmi". According to Table 2.2, there are 161 patients who had stroke and 40 patients who have not had stroke in patients with their BMI value missing. We decide to remove these rows with missing values in BMI.

```{r dealing with missing data}
## Remove NAs
stroke<-na.omit(originalStroke)
```

```{r stroke class}
#View Classese of stroke
stroke_classes<-ggplot(data = originalStroke, mapping = aes(x=stroke,fill=stroke))+
  geom_bar(xlab="Stroke Status")+
  xlab("Stroke Status")+
  ggtitle("Figure 2.2 Classes of Stroke")
  
stroke_classes
```




```{r dealing with imbalance of data set}
#Utilizing smote_nc to synthesize additional data points for the minority class in the data set.
set.seed(114514)
test<-SMOTE_NC(stroke,"stroke",perc_maj = 25,5)
#write.csv(test, file = "Smote0.25.csv")
#test <- as.data.frame(unclass(test),stringsAsFactors=TRUE)

```

We can see from Figure 2.2 that the class of stroke status was imbalanced, therefore, we utilized the technique called Synthetic Minority Oversampling Technique(SMOTE) to synthesize data points for the minority class, therefore increase the proportion of the minority class in the data set and enable us to build an usable classifier

The SMOTE technique is an oversampling technique that utilizes the technique called K-Nearest Neighbor algorithm to synthesize data.The SMOTE randomly choose K rows of data from the minority class in the data set and synthesize new data points on the line segment between each of the two data points.

This technique allows us to mitigate the imbalance in the data set with less concern on the issue of overfitting in regular oversampling, which make the model giving accurate predictions on training data but unable to making correct prediction on new data provided(Wijaya).

An experimental package from github was utilized to conduct SMOTE for our datat set that consists of both categorical and numeric features(Wu).





### EDA

```{r correlation plot for numeric features}
M<-cor(test[,c(3,9,10)])
#Create correlation plot
corrplot(M, method= "circle",type = "upper",title="Figure 2.x Correlation Plot of Numeric Variables in Stroke Data Set ",mar = c(0,0,2,0))
```

```{r}
heart_smoking<-table(test$heart_disease,test$smoking_status)
heart_smoking
```

```{r}
prop.table(heart_smoking)
```
```{r}
prop.table(heart_smoking,1)
```

We can see from the Figure 2.3 that correlations present between the numeric features in the data set, indicating that multicolinearity exists across features, therefore utilizing penalized regression models(ridge, lasso, elastic net) were appropriate for the data set.




```{r empirical logit function, include=FALSE}
#Load function for empirical logit plot
emplogitPlot <- function(x, y, binsize = NULL, ci = FALSE, probit = FALSE,
prob = FALSE, main = NULL, xlab = "", ylab = "", lowess.in = FALSE){
  
  if(class(y) =="character"){
   y <- as.numeric(as.factor(y))-1
   }
  
  if (length(x) != length(y))
    stop("x and y lengths differ")
  if (any(y < 0 | y > 1))
    stop("y not between 0 and 1")
  if (length(x) < 100 & is.null(binsize))
    stop("Less than 100 observations: specify binsize manually")
  
  if (is.null(binsize)) binsize = min(round(length(x)/10), 50)
  
  if (probit){
    link = qnorm
    if (is.null(main)) main = "Empirical probits"
  } else {
    link = function(x) log(x/(1-x))
    if (is.null(main)) main = "Empirical logits"
  }
  
  sort = order(x)
  x = x[sort]
  y = y[sort]
  a = seq(1, length(x), by=binsize)
  b = c(a[-1] - 1, length(x))
  
  prob = xmean = ns = rep(0, length(a)) # ns is for CIs
  for (i in 1:length(a)){
    range = (a[i]):(b[i])
    prob[i] = mean(y[range])
    xmean[i] = mean(x[range])
    ns[i] = b[i] - a[i] + 1 # for CI 
  }
  
  extreme = (prob == 1 | prob == 0)
  prob[prob == 0] = min(prob[!extreme])
  prob[prob == 1] = max(prob[!extreme])
  
  g = link(prob) # logits (or probits if probit == TRUE)
  
  linear.fit = lm(g[!extreme] ~ xmean[!extreme])
  b0 = linear.fit$coef[1]
  b1 = linear.fit$coef[2]
  
  loess.fit = loess(g[!extreme] ~ xmean[!extreme])
  
  plot(xmean, g, main=main, xlab=xlab, ylab=ylab)
  abline(b0,b1)
  if(lowess.in ==TRUE){
  lines(loess.fit$x, loess.fit$fitted, lwd=2, lty=2)
  }
}
```






## Section 3: Method 1:K Nearest Neighbor

### Section 3.1: Introduction

We begin building the classifier by implementing a technique called K-Nearest Neighbor(KNN) to predict the stroke status of the patients.It uses the stroke status of the k nearest neighbor data points to predict the stroke status of any given data point in the data set. Figure 3.1 is an illustration of this technique.

```{r}
set.seed(114514)
numRow<-nrow(test)
chosenDemo<-sample(1:numRow,200,replace = FALSE)
set_demo<-test[chosenDemo,]
```


```{r knn demo illustration}
demo_data<-data.frame("Age"= rep(NA,1),"BMI"=rep(NA,1),"Stroke"=rep(NA,1))
demo_data$Age[1]<-69
demo_data$BMI[1]<-22.8
demo_data$Stroke<-1

#demoTest<-example_data%>%
 # mutate(dengue_status_test=as.character(dengue_status))
ggplot()+
  geom_point(data = set_demo,aes(x=age,y=bmi,col=stroke),size=2,alpha=0.9,shape=19)+
  geom_point(data =demo_data,aes(x=Age,y=BMI),size=2,alpha=0.9,shape=4)
```


### Section 3.2: Method

```{r KNN Data}
#Create a data set with only numeric features for the KNN
numericOnly<-test[,c(3,9,10,12)]
```

```{r 10 fold CV determine best k }
set.seed(114514)
n<-nrow(numericOnly)
nk<-50
storage<-data.frame("K"=rep(NA,nk),
                    "Sensitivity" = rep(NA,nk),
                    "Specificity" = rep(NA,nk),
                    "GeoMean" = rep(NA,nk))

#Set a seed for sampling and create a pool and set fold for K fold CV
set.seed(114514)
pool<-rep(1:10,ceiling(n/10))

fold<-sample(pool,n,replace = FALSE)

#Outer loop for increment k 
for(k in 1:nk){
  storage$K[k]<-k
  
  storage_inner<-data.frame("YHat" = rep(NA,n))
  #Inner loop for 10 fold CV
  for(i in 1:10){
    #Find data in each fold
    infold<-which(fold == i)
    
    #Create training and testing sets
    Train_Stroke<-numericOnly[-infold,]
    Test_Stroke<-numericOnly[infold,]
   #(set.seed(114514))
    #Run Knn
    k_preds<-knn(Train_Stroke[,c(1,2,3)],Test_Stroke[,c(1,2,3)],k=k,cl=Train_Stroke$stroke)
    
    #store predicted result from each fold to storage_inner and obtain predictions to     the full data set 
    storage_inner$YHat[infold]<-as.numeric(as.character(k_preds))
  }
    #Find rows with positive and negative result
    true1K<-which(numericOnly$stroke == 1)
    true0K<-which(numericOnly$stroke == 0)
    
    #Compute the amount of rows corresponding to each result
    ntrue1K<-length(true1K)
    ntrue0K<-length(true0K)
    #Compute sensitivity and specificity, as well as GeoMetric Mean for determining the best value for k
    sensitivity<-sum(storage_inner$YHat[true1K] == 1)/ntrue1K
    storage$Sensitivity[k]<-sensitivity
    specificity<-sum(storage_inner$YHat[true0K] == 0)/ntrue0K
    storage$Specificity[k]<-specificity
    storage$GeoMean[k]<-sqrt(sensitivity*specificity)
    
    #if(k==8){
     # YHatOut <- storage_inner$YHat
    #}
   
}

```

```{r find best k}
#Create plot to find the best k for KNN 
knnActualPlot<-ggplot(storage,aes(K,GeoMean))+
  geom_line()+
  labs(caption = paste("Geometric Mean, ReD Line at K=", which.max(storage$GeoMean)),title = "Figure 2.3: GeoMean Graph of KNN with Best k Shown
  (10 Fold Cross Validation)",y = " ")+
  geom_vline(xintercept = which.max(storage$GeoMean),lty = 2,col="red")
#Show Plot
knnActualPlot
```

### Section 3.3: Results

## Section 4: Method 2: Penalized Regression(Ridge, Lasso and Elastic Net)

Figure 2.1 indicated that the correlations between numeric and categorical features exists, therefore, we decide to implement penalized regression models to mitigate this situation, since 

### Section 4.1: Introduction

### Section 4.2: Method


```{r}
test_noID<-test[,-1]
```

```{r creating design matrix ridge}
# Create the design matrix for ridge regression
XD <- model.matrix(stroke ~., data = test_noID)
```

```{r cv to find the best lambda}
#Set seed for random sampling
set.seed(114514)
#Run cross validation for lambda value from 1 to 50(increment by 0.05 for each run)
ridge.modFind_Lambda <- cv.glmnet(XD[,-11], test_noID$stroke , alpha = 0,standardize = TRUE,family = "binomial")
```

```{r}
CV_result_ridge<-data.frame("Lambda" = ridge.modFind_Lambda$lambda,"Deviance" = ridge.modFind_Lambda$cvm)
smallestRidge_Deviance<-which.min(CV_result_ridge$Deviance)
knitr::kable(CV_result_ridge[smallestRidge_Deviance,],caption = "Table 4.x Deviance of Model with Chosen Lambda(Ridge)")
```

```{r ridge final and pred}
ridge.final<-glmnet(XD[,-11], test_noID$stroke , alpha = 0,lambda = CV_result_ridge[smallestRidge_Deviance,]$Lambda	, standardize = TRUE,family = "binomial")
pred_ridge<-predict(ridge.final,newx=XD[,-11],type = "response")
```

```{r ROCAUC ridge}
roc_ridge<-roc(test$stroke,pred_ridge)
plot(roc_ridge)
knitr::kable(auc(roc_ridge))
holder <- data.frame("Threshold" = roc_ridge$thresholds, "Sensivity" = roc_ridge$sensitivities, "Spec" = roc_ridge$specificities, "GMean" = sqrt(roc_ridge$sensitivities*roc_ridge$specificities))
holder[which.max(holder$GMean),]
```

```{r prediction with adjuste threshold}
pred_ridge_Y<-ifelse(pred_ridge>holder[which.max(holder$GMean),]$Threshold,"1","0")
```

```{r ridge mtx2}
knitr::kable(table("Prediction" = pred_ridge_Y,"Actual"= test$stroke),caption = "Table 4.x Confusion Matrix for Prediction of Stroke Via Ridge")
```

## Lasso

```{r find best lambda}
lasso.modFind_Lambda <- cv.glmnet(XD[,-11], test_noID$stroke , alpha = 1, standardize = TRUE,family = "binomial")
```

```{r}
CV_result_lasso<-data.frame("Lambda" = lasso.modFind_Lambda$lambda,"Deviance" = lasso.modFind_Lambda$cvm)
smallestLasso_Deviance<-which.min(CV_result_lasso$Deviance)
knitr::kable(CV_result_lasso[smallestLasso_Deviance,],caption = "Table 4.x Deviance of Model with Chosen Lambda(Lasso)")
```

```{r lasso final}
lasso.final<-glmnet(XD[,-11], test_noID$stroke , alpha = 1,lambda = CV_result_lasso[which.min(CV_result_lasso$Deviance),]$Lambda	, standardize = TRUE,family = "binomial")
pred_lasso<-predict(lasso.final,newx=XD[,-11],type = "response")

```

```{r ROC AUC Lasso}
roc_lasso<-roc(test$stroke,pred_lasso)
plot(roc_lasso)
auc(roc_lasso)
holder_lasso <- data.frame("Threshold" = roc_lasso$thresholds, "Sensivity" = roc_lasso$sensitivities, "Spec" = roc_lasso$specificities, "GMean" = sqrt(roc_lasso$sensitivities*roc_lasso$specificities))
```

```{r lasso prediction with adjusted threshold}
predicted.Y_lasso<-ifelse(pred_lasso>holder_lasso[which.max(holder_lasso$GMean),]
$Threshold,"1","0")

```

```{r lasso mtx}
knitr::kable(table("Prediction" = predicted.Y_lasso,"Actual"= test$stroke),caption = "Table 4.x Confusion Matrix for Prediction of Stroke Via Lasso")
```

```{r adjusted metrics lasso}
knitr::kable(holder_lasso[which.max(holder_lasso$GMean),],caption = "Table 4.x ")
```

```{r find alpha and lambda with smallest deviance elastic net}
#Set a sequence of numbers from 0 to 1, increment by 0.01 each time 
seq_alpha<-seq(from = 0, to = 1, by = 0.01)
#Create empty data frame to hold alpha, lambda and RMSE of each model fitted with corresponding tuning parameter
storageElastic<-data.frame("Alpha" = rep(NA,length(seq_alpha)), "Lambda" = rep(NA,length(seq_alpha)),"Deviance" = rep(NA, length(seq_alpha)))

aph=1
#Set seed for random sampling 
set.seed(1)
#Loop to test each lambda and alpha and save MSE, RMSE with corresponding lambda and alpha to a dataframe
for(i in seq_alpha){
  Elastic.modFind.ideal<-cv.glmnet(XD[,-11], test_noID$stroke,alpha = i,family="binomial")
  storageElastic$Lambda[aph]<-Elastic.modFind.ideal$lambda.min
  storageElastic$Alpha[aph]<-i
  storageElastic$Deviance[aph]<-min(Elastic.modFind.ideal$cvm)
  aph=aph+1
}

```

```{r elastic metrics}
#Get the smallest RMSE value as well as corresponding lambda and alpha
smallestElastic_Deviance<-which.min(storageElastic$Deviance)
knitr::kable(storageElastic[smallestElastic_Deviance,],caption = "Table 5.1 Smallest Deviance with Corresponding Alpha and Lambda")

```

```{r}
ideal_set<-storageElastic[smallestElastic_Deviance,]
alpha_chosen<-ideal_set$Alpha
lambda_chosen<-ideal_set$Lambda
```

```{r elastic net finla model}
Elastic.Final.Model<-glmnet(XD[,-11], test_noID$stroke,alpha = alpha_chosen,lambda = lambda_chosen,family="binomial")
```

```{r elastic net prediction}
pred_elastic<-predict(Elastic.Final.Model,newx = XD[,-11],type = "response")
```

```{r ROCAUC elastic net}
roc_elastic<-roc(test$stroke,pred_elastic)
plot(roc_elastic)
auc(roc_elastic)
holder_elastic <- data.frame("Threshold" = roc_elastic$thresholds, "Sensivity" = roc_elastic$sensitivities, "Spec" = roc_elastic$specificities, "GMean" = sqrt(roc_elastic$sensitivities*roc_elastic$specificities))

holder_elastic[which.max(holder_elastic$GMean),]
```

```{r elastic net prediction with adjusted threshold}
#Make prediction with adjusted threshold and create confusion matrix for it
pred_Y_elastic<-ifelse(pred_elastic>holder_elastic[which.max(holder_elastic$GMean),]$Threshold,"1","0")
knitr::kable(table("Prediction" = pred_Y_elastic,"Actual"= test$stroke),caption = "Confusion Matrix for Prediction of Stroke Via Elastic Net")
```

### Section 4.3: Results:

## Section 5: Method 3

### Section 5.1: Introduction

### Section 5.2: Method

```{r bagged forest}
#Set seed 
set.seed(100)
set_for_forest<-test[,-1]
bagged<-randomForest(stroke~.,data=test,mtry=10,importance=TRUE,ntree=1000,compete=FALSE)
```

```{r bagged prediction}
#bag forest prediction
for_predict<-test[,-c(1,12)]
pred_bag<-predict(bagged)
table(pred_bag)
table("Prediction"= pred_bag,"Actual" = test$stroke)
```

```{r random forest}
set.seed(100)
random<-randomForest(stroke~.,data=test,mtry=sqrt(10),importance=TRUE,ntree=1000,compete=FALSE)
```

```{r bagged confusion matrix}
#Formatted Confusion Matrix for bagge forest
knitr::kable(table("Prediction"=bagged$predicted,"Actual"=test$stroke),caption="Table 5.x: Confusion Matrix of Bagged Forest")
```

$$Sensitivity=\frac{785}{390+785}\approx0.668$$
$$Specificity=\frac{4511}{4511+189}\approx0.960$$ 
$$Accuracy=\frac{4511+785}{5875}\approx0.901$$

$$CER=1-Accuracy=0.099$$

```{r Random Forest Confusion Matrix}
knitr::kable(table("Prediction"=random$predicted,"Actual"=test$stroke),caption="Table 5.x: Confusion Matrix of Random Forest")
```

$$Sensitivity=\frac{750}{425+750}\approx0.638$$

$$Specificity=\frac{4532}{4532+168}\approx0.964$$

$$Accuracy=\frac{4532+750}{5875}\approx0.899$$ 
$$CER=1-Accuracy=1-0.899=0.101$$

## Conclusion

## Works Cited

Fedesoriano. "Stroke Prediction Dataset." Kaggle, 26 Jan. 2021, <https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset/code>.

Wijaya, Cornellius Yudha. "5 Smote Techniques for Oversampling Your Imbalance Data." Medium, Towards Data Science, 24 May 2022, <https://towardsdatascience.com/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5>.

RolandRoland."Convert All Data Frame Character Columns to Factors." Stack Overflow, 17 Dec. 2013, <https://stackoverflow.com/questions/20637360/convert-all-data-frame-character-columns-to-factors>.

Wu, Dongyuan. "Dongyuanwu/RSBID: Resampling Strategies for Binary Imbalanced Datasets Version 0.0.2.0000 from Github." Version 0.0.2.0000 from GitHub, 18 July 2022, <https://rdrr.io/github/dongyuanwu/RSBID/>.

Singh, Deepika. “Deepika Singh.” Pluralsight, 12 Nov. 2019, https://www.pluralsight.com/guides/finding-relationships-data-with-r. 

Reka Solymosi (maintained and updated by David Buil-Gil and Nicolas Trajtenberg). “Making Sense of Crim Data.” Chapter 3 Week 3, 8 Nov. 2022, https://maczokni.github.io/MSCD_labs/week3.html. 