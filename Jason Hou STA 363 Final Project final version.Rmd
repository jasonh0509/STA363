---
title: "Jason Hou STA 363 Final Project"
author: "Jueshen Hou"
date: '2022-11-25'
output: html_document
---

```{r load packages, include=FALSE}
##Here we load the packages needed and install a package that is not on CRAN
library(ggplot2)
library(broom)
library(gridExtra)
library(dplyr)
library(class)
library(tidyverse)
library(gridExtra)
library(leaps)
library(corrplot)
library(RColorBrewer)
library(glmnet)
library(xtable)
library(RSBID)
library(randomForest)
library(pROC)
#library(smotefamily)
library(devtools)
devtools::install_github("dongyuanwu/RSBID")
library(RSBID)
library(UpSetR)
library(naniar)
library(report)
```

## Abstract

## Section 1: Data and Motivation



The data that we are dealing on is a data consist of 5110 rows and 12 columns.The data set record the 12 features of 5110 suspected stroke patients. We have the information on the patient's biological gender, the age of each patient in years,whether a patient has heart disease/hypertension, marital status, smoking status, stroke status and other related information. For other features not mentioned, please refer to the following link for a detailed description of features in this data set.(Fedesoriano)
Link: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset

Our goal is to build a classifier that helps to predict stroke event based on the health conditions and related information provided. 
## Section 2: Data Cleaning and EDA


```{r load data}
originalStroke<-read.csv("healthcare-dataset-stroke-data.csv")
```

```{r include=FALSE}
glimpse(originalStroke)
```

```{r dealing with missing data}
# Check missing data
knitr::kable(sum(is.na(originalStroke))
,caption = "Table 2.1 Number of Missing Values")
## Remove NAs
stroke<-na.omit(originalStroke)
```

```{r change data type}
stroke$bmi<-as.numeric(stroke$bmi)
stroke$stroke<-as.factor(stroke$stroke)
```



```{r}
#View Classese of stroke
stroke_classes<-ggplot(data = stroke, mapping = aes(x=stroke,fill=stroke))+
  geom_bar(xlab="Stroke Status")+
  xlab("Stroke Status")+
  ggtitle("Figure 2.1 Classes of Stroke")
  
  
stroke_classes
```

We can see from Figure 2.1 that the class of stroke status was imbalanced, therefore, we utilized the technique called Synthetic Minority Oversampling Technique(SMOTE) to synthesize data points for the minority class, therefore increase the proportion of the minority class in the data set.

The SMOTE technique 
(Wijaya)

An experimental pacakge from github was utilized to conduct SMOTE for our datat set that consists of both categorical and numeric features(Wu).


```{r dealing with missing data}
#load original data set
originalStroke$bmi<-as.numeric(originalStroke$bmi)
## Remove NAs
stroke<-na.omit(originalStroke)
```

```{r change data type}
#Change all categorical data that are in character type to factor type
stroke <- as.data.frame(unclass(stroke),stringsAsFactors=TRUE)
```

```{r dealing with imbalance of data set}
#Utilizing smote_nc to synthesize additional data points for the minority class in the data set.
set.seed(114514)
test<-SMOTE_NC(stroke,"stroke",perc_maj = 25,5)
#write.csv(test, file = "Smote0.25.csv")
```


```{r collapsing }
stroke<-stroke%>%
  transmute(
    
  )
```


## Section 3: Method 1:K Nearest Neighbor

### Section 3.1: Introduction
We begin by implementing a technique called K-Nearest Neighbor(KNN) to predict the stroke status of the patients.It  

### Section 3.2: Method

```{r KNN Data}
#Create a data set with only numeric features for the KNN
numericOnly<-test[,c(3,9,10,12)]
```

```{r 10 fold CV determine best k }
set.seed(114514)
n<-nrow(numericOnly)
nk<-50
storage<-data.frame("K"=rep(NA,nk),
                    "Sensitivity" = rep(NA,nk),
                    "Specificity" = rep(NA,nk),
                    "GeoMean" = rep(NA,nk))

#Set a seed for sampling and create a pool and set fold for K fold CV
set.seed(114514)
pool<-rep(1:10,ceiling(n/10))

fold<-sample(pool,n,replace = FALSE)

#Outer loop for increment k 
for(k in 1:nk){
  storage$K[k]<-k
  
  storage_inner<-data.frame("YHat" = rep(NA,n))
  #Inner loop for 10 fold CV
  for(i in 1:10){
    #Find data in each fold
    infold<-which(fold == i)
    
    #Create training and testing sets
    Train_Stroke<-numericOnly[-infold,]
    Test_Stroke<-numericOnly[infold,]
   (set.seed(114514))
    #Run Knn
    k_preds<-knn(Train_Stroke[,c(1,2,3)],Test_Stroke[,c(1,2,3)],k=k,cl=Train_Stroke$stroke)
    
    #store predicted result from each fold to storage_inner and obtain predictions to     the full data set 
    storage_inner$YHat[infold]<-as.numeric(as.character(k_preds))
  }
    #Find rows with positive and negative result
    true1K<-which(numericOnly$stroke == 1)
    true0K<-which(numericOnly$stroke == 0)
    
    #Compute the amount of rows corresponding to each result
    ntrue1K<-length(true1K)
    ntrue0K<-length(true0K)
    #Compute sensitivity and specificity, as well as GeoMetric Mean for determining the best value for k
    sensitivity<-sum(storage_inner$YHat[true1K] == 1)/ntrue1K
    storage$Sensitivity[k]<-sensitivity
    specificity<-sum(storage_inner$YHat[true0K] == 0)/ntrue0K
    storage$Specificity[k]<-specificity
    storage$GeoMean[k]<-sqrt(sensitivity*specificity)
    
    #if(k==8){
     # YHatOut <- storage_inner$YHat
    #}
   
}

```

```{r find best k}
#Create plot to find the best k for KNN 
knnActualPlot<-ggplot(storage,aes(K,GeoMean))+
  geom_line()+
  labs(caption = paste("Geometric Mean, ReD Line at K=", which.max(storage$GeoMean)),title = "Figure 2.3: GeoMean Graph of KNN with Best k Shown
  (10 Fold Cross Validation)",y = " ")+
  geom_vline(xintercept = which.max(storage$GeoMean),lty = 2,col="red")
#Show Plot
knnActualPlot
```

### Section 3.3: Results

## Section 4: Method 2:

### Section 4.1: Introduction

### Section 4.2: Method

```{r}

```

```{r}
test_noID<-test[,-1]
```

```{r creating design matrix ridge}
# Create the design matrix for ridge regression
XD <- model.matrix(stroke ~., data = test_noID)
```

```{r cv to find the best lambda}
#Set seed for random sampling
set.seed(114514)
#Run cross validation for lambda value from 1 to 50(increment by 0.05 for each run)
ridge.modFind_Lambda <- cv.glmnet(XD[,-11], test_noID$stroke , alpha = 0,standardize = TRUE,family = "binomial")
```

```{r}
CV_result_ridge<-data.frame("Lambda" = ridge.modFind_Lambda$lambda,"Deviance" = ridge.modFind_Lambda$cvm)
smallestRidge_Deviance<-which.min(CV_result_ridge$Deviance)
knitr::kable(CV_result_ridge[smallestRidge_Deviance,],caption = "Table 4.x Deviance of Model with Chosen Lambda(Ridge)")
```

```{r ridge final and pred}
ridge.final<-glmnet(XD[,-11], test_noID$stroke , alpha = 0,lambda = 0.0181107	, standardize = TRUE,family = "binomial")
pred_ridge<-predict(ridge.final,newx=XD[,-11],type = "response")
```


```{r ROCAUC ridge}
roc_ridge<-roc(test$stroke,pred_ridge)
plot(roc_ridge)
knitr::kable(auc(roc_ridge))
holder <- data.frame("Threshold" = roc_ridge$thresholds, "Sensivity" = roc_ridge$sensitivities, "Spec" = roc_ridge$specificities, "GMean" = sqrt(roc_ridge$sensitivities*roc_ridge$specificities))
holder[which.max(holder$GMean),]
```

```{r prediction with adjuste threshold}
pred_ridge_Y<-ifelse(pred_ridge>holder[which.max(holder$GMean),]$Threshold,"1","0")
```

```{r ridge mtx2}
knitr::kable(table("Prediction" = pred_ridge_Y,"Actual"= test$stroke),caption = "Table 4.x Confusion Matrix for Prediction of Stroke Via Ridge")
```



## Lasso

```{r find best lambda}
lasso.modFind_Lambda <- cv.glmnet(XD[,-11], test_noID$stroke , alpha = 1, standardize = TRUE,family = "binomial")
```

```{r}
CV_result_lasso<-data.frame("Lambda" = lasso.modFind_Lambda$lambda,"Deviance" = lasso.modFind_Lambda$cvm)
smallestLasso_Deviance<-which.min(CV_result_lasso$Deviance)
knitr::kable(CV_result_lasso[smallestLasso_Deviance,],caption = "Table 4.x Deviance of Model with Chosen Lambda(Lasso)")
```

```{r lasso final}
lasso.final<-glmnet(XD[,-11], test_noID$stroke , alpha = 1,lambda = 0.0015729	, standardize = TRUE,family = "binomial")
pred_lasso<-predict(lasso.final,newx=XD[,-11],type = "response")

```

```{r ROC AUC Lasso}
roc_lasso<-roc(test$stroke,pred_lasso)
plot(roc_lasso)
auc(roc_lasso)
holder_lasso <- data.frame("Threshold" = roc_lasso$thresholds, "Sensivity" = roc_lasso$sensitivities, "Spec" = roc_lasso$specificities, "GMean" = sqrt(roc_lasso$sensitivities*roc_lasso$specificities))
```


```{r lasso prediction with adjusted threshold}
predicted.Y_lasso<-ifelse(pred_lasso>holder_lasso[which.max(holder_lasso$GMean),]
$Threshold,"1","0")

```




```{r lasso mtx}
knitr::kable(table("Prediction" = predicted.Y_lasso,"Actual"= test$stroke),caption = "Table 4.x Confusion Matrix for Prediction of Stroke Via Lasso")
```

```{r adjusted metrics lasso}
knitr::kable(holder_lasso[which.max(holder_lasso$GMean),],caption = "Table 4.x ")
```


```{r find alpha and lambda with smallest deviance elastic net}
#Set a sequence of numbers from 0 to 1, increment by 0.01 each time 
seq_alpha<-seq(from = 0, to = 1, by = 0.01)
#Create empty data frame to hold alpha, lambda and RMSE of each model fitted with corresponding tuning parameter
storageElastic<-data.frame("Alpha" = rep(NA,length(seq_alpha)), "Lambda" = rep(NA,length(seq_alpha)),"Deviance" = rep(NA, length(seq_alpha)))

aph=1
#Set seed for random sampling 
set.seed(1)
#Loop to test each lambda and alpha and save MSE, RMSE with corresponding lambda and alpha to a dataframe
for(i in seq_alpha){
  Elastic.modFind.ideal<-cv.glmnet(XD[,-11], test_noID$stroke,alpha = i,family="binomial")
  storageElastic$Lambda[aph]<-Elastic.modFind.ideal$lambda.min
  storageElastic$Alpha[aph]<-i
  storageElastic$Deviance[aph]<-min(Elastic.modFind.ideal$cvm)
  aph=aph+1
}

```

```{r elastic metrics}
#Get the smallest RMSE value as well as corresponding lambda and alpha
smallestElastic_Deviance<-which.min(storageElastic$Deviance)
knitr::kable(storageElastic[smallestElastic_Deviance,],caption = "Table 5.1 Smallest Deviance with Corresponding Alpha and Lambda")

```

```{r}
ideal_set<-storageElastic[smallestElastic_Deviance,]
alpha_chosen<-ideal_set$Alpha
lambda_chosen<-ideal_set$Lambda
```



```{r elastic net finla model}
Elastic.Final.Model<-glmnet(XD[,-11], test_noID$stroke,alpha = alpha_chosen,lambda = lambda_chosen,family="binomial")
```

```{r elastic net prediction}
pred_elastic<-predict(Elastic.Final.Model,newx = XD[,-11],type = "response")
```

```{r ROCAUC elastic net}
roc_elastic<-roc(test$stroke,pred_elastic)
plot(roc_elastic)
auc(roc_elastic)
holder_elastic <- data.frame("Threshold" = roc_elastic$thresholds, "Sensivity" = roc_elastic$sensitivities, "Spec" = roc_elastic$specificities, "GMean" = sqrt(roc_elastic$sensitivities*roc_elastic$specificities))
which.max(holder_elastic$GMean)
holder_elastic[3715,]
```

```{r elastic net prediction with adjusted threshold}
#Make prediction with adjusted threshold and create confusion matrix for it
pred_Y_elastic<-ifelse(pred_elastic>holder_elastic[3715,]$Threshold,"1","0")
knitr::kable(table("Prediction" = pred_Y_elastic,"Actual"= test$stroke),caption = "Confusion Matrix for Prediction of Stroke Via Elastic Net")
```


### Section 4.3: Results:

## Section 5: Method 3

### Section 5.1: Introduction

### Section 5.2: Method

```{r bagged forest}
#Set seed 
set.seed(100)
set_for_forest<-test[,-1]
bagged<-randomForest(stroke~.,data=test,mtry=10,importance=TRUE,ntree=1000,compete=FALSE)
```

```{r bagged prediction}
#bag forest prediction
for_predict<-test[,-c(1,12)]
pred_bag<-predict(bagged)
table(pred_bag)
table("Prediction"= pred_bag,"Actual" = test$stroke)
```

```{r random forest}
set.seed(100)
random<-randomForest(stroke~.,data=test,mtry=sqrt(10),importance=TRUE,ntree=1000,compete=FALSE)
```


```{r bagged confusion matrix}
#Formatted Confusion Matrix for bagge forest
knitr::kable(table("Prediction"=bagged$predicted,"Actual"=test$stroke),caption="Table 5.x: Confusion Matrix of Bagged Forest")
```


$$Sensitivity=\frac{845}{330+845}\approx0.719$$
$$Specificity=\frac{4616}{4616+84}\approx0.982$$ 
$$Accuracy=\frac{4616+845}{5875}\approx0.93$$

$$CER=1-Accuracy=0.07$$

```{r Random Forest Confusion Matrix}
knitr::kable(table("Prediction"=random$predicted,"Actual"=test$stroke),caption="Table 5.x: Confusion Matrix of Random Forest")
```

$$Sensitivity=\frac{805}{370+805}\approx0.69$$

$$Specificity=\frac{4637}{4637+63}\approx0.987$$

$$Accuracy=\frac{4637+805}{5875}\approx0.926$$ 
$$CER=1-Accuracy=1-0.926=0.074$$



## Conclusion

## Works Cited

Fedesoriano. "Stroke Prediction Dataset." Kaggle, 26 Jan. 2021, <https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset/code>.

Wijaya, Cornellius Yudha. “5 Smote Techniques for Oversampling Your Imbalance Data.” Medium, Towards Data Science, 24 May 2022, https://towardsdatascience.com/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5. 

RolandRoland.“Convert All Data Frame Character Columns to Factors.” Stack Overflow, 17 Dec. 2013, https://stackoverflow.com/questions/20637360/convert-all-data-frame-character-columns-to-factors.

Wu, Dongyuan. “Dongyuanwu/RSBID: Resampling Strategies for Binary Imbalanced Datasets Version 0.0.2.0000 from Github.” Version 0.0.2.0000 from GitHub, 18 July 2022, https://rdrr.io/github/dongyuanwu/RSBID/. 