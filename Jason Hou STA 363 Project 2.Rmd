---
title: "Jason Hou STA 363 Project 2"
author: "Jason Hou"
date: '2022-10-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages}
##Here we load the packages needed
library(ggplot2)
library(broom)
library(gridExtra)
library(dplyr)
library(class)
library(tidyverse)
library(gridExtra)
library(leaps)
library(corrplot)
library(RColorBrewer)
library(glmnet)
library(xtable)
```
# Abstract

## Part 1 Introduction of Data

```{r load data}
vrboData<-read.csv("VRBO.csv")
```

```{r take a look}
glimpse(vrboData)
```


## Part 2: Data Cleaning

```{r checking and cleaning}
# Find out the number of missing values in the vrbo data set
sum(is.na(vrboData))
#Remove rows with missing values 


vrboCleaned<-vrboData%>%
  na.omit(vrboData)%>%
  select(-UnitNumber)
vrboCleaned$PctRentals=as.numeric((vrboCleaned$PctRentals)*100)
```

```{r distribution}
ggplot(data=vrboCleaned,aes(x=price))+
  geom_boxplot()+
  ggtitle("Figure 2.1 Distribution of Price")
```


We can see that 

```{r}
knitr::kable(table(vrboData$room_type))
```

We identified that the UnitNumber is a feature that should be removed from the data set before building the model, as it has 1423 different levels, and is irrelevant to the price of the vrbo rentals.A new data set "VrboCleaned" was created for the following sections. 


```{r convert data type}
#convert character features into factor
vrboCleaned$neighborhood <-as.factor(vrboCleaned$neighborhood)
```

```{r correlation plot}
#Obtain correlation between features and save it as an object
M<-cor(vrboCleaned[,-c(4,8,9)])
#Create correlation plot
corrplot(M, method= "circle",type = "upper",title="Figure 2.2 Correlation Plot of Numeric Variables in Vrbo Data Set ",mar = c(0,0,2,0))
#M
```

We created a correlation plot for numeric features in the data set to explore if there are potential correlations present between features that allows us to determine the proper regression methods for building the model. Figure 2.1 shows that several significant correlations are presented between features, indicating shrinkage and selection techniques are appropriate to be used on building the model.








## Part 3: Ridge Regression


```{r creating design matrix ridge}
# Create the design matrix for ridge regression
XD <- model.matrix(price ~., data = vrboCleaned)
```





```{r include=FALSE}
set.seed(1)
ridge.modFind_Lambda <- cv.glmnet(XD[,-1], vrboCleaned$price , alpha = 0 , lambda = seq(from=0, to=50,by=0.05) , standardize = TRUE)
```


```{r load a function, include=FALSE}
##Load a function for creating the plot of RMSE in response of change in Lambda
ridgePlot <- function(ridge.mod, metric, title){
  library(ggplot2)
  
  smallestLambda <- ridge.mod$lambda[which.min(ridge.mod$cvm)] 
  
  if(metric == "MSE"){
  g1 <- ggplot( data.frame(ridge.mod$lambda), aes( x = ridge.mod$lambda, y = (ridge.mod$cvm))) + geom_point() + geom_vline( xintercept = smallestLambda, col = "blue" , lty = 2 ) +  labs(caption = paste("Test MSE values for Different Tuning Parameters. Smallest MSE at lambda = ", smallestLambda), title = title, y = "Test MSE", x = "Tuning Parameter")
  
  }
  
  if(metric == "RMSE"){
  g1 <- ggplot( data.frame(ridge.mod$lambda), aes( x = ridge.mod$lambda, y = sqrt(ridge.mod$cvm))) + geom_point() + geom_vline( xintercept = smallestLambda, col = "blue" , lty = 2 ) +  labs(caption = paste("Test RMSE values for Different Tuning Parameters. Smallest RMSE at lambda = ", smallestLambda), title = title, y = "Test RMSE", x = "Tuning Parameter")

  }
  
  g1
}
```


```{r plot to determine the best lambda}
#Using the ridgePlot function(immported) to creat graph of RMSE in response to Lambda
ridgePlot(ridge.modFind_Lambda, metric = "RMSE" , title ="Figure 3.1 RMSE in Response to Lambda(Ridge)" )

```




```{r ridge}
ridge.mod_Final <- glmnet(XD[,-1], vrboCleaned$price , alpha = 0 , lambda = 13.1 , standardize = TRUE)
Betas_ridge<-as.numeric(coefficients(ridge.mod_Final))
ridge_coef<-data.frame("Coefficients" = Betas_ridge)

rownames(ridge_coef)<-colnames(XD)
knitr::kable(ridge_coef,caption = "Table 3.1 Coefficients of Model using Ridge Regression")

```

```{r ridge metrics}
CV_result_ridge<-data.frame("Lambda" = ridge.modFind_Lambda$lambda,"MSE" = ridge.modFind_Lambda$cvm,"RMSE" = sqrt(ridge.modFind_Lambda$cvm))
smallestRidge_MSE<-which.min(CV_result_ridge$MSE)
smallestRidge_RMSE<-which.min(CV_result_ridge$RMSE)
CV_result_ridge[smallestRidge_MSE,]
CV_result_ridge[smallestRidge_RMSE,]
```



## Part 4: Lasso

```{r}
#Set seed for random sampling process within the cv.glmnet
set.seed(1)
#Fit modesls for a serie of lambda values, respectively 
Lasso.modFind_Lambda <- cv.glmnet(XD[,-1], vrboCleaned$price , alpha = 1 , lambda = seq(from=0, to=50,by=0.05) , standardize = TRUE)

#Plot the change of RMSE in reponse to lambda values
ridgePlot(Lasso.modFind_Lambda, metric = "RMSE" , title ="Figure 3.2 RMSE in Response to Lambda(Lasso)" )

```

```{r lasso final}
lasso.mod_Final <- glmnet(XD[,-1], vrboCleaned$price , alpha = 1 , lambda = 0.75 , standardize = TRUE)
Betas_lasso<-as.numeric(coefficients(lasso.mod_Final))
lasso_coef<-data.frame("Coefficients" = Betas_lasso)

rownames(lasso_coef)<-colnames(XD)
knitr::kable(lasso_coef,caption = "Table 4.1 Coefficients of Model using Lasso Regression")

```

```{r lasso metrics} 
CV_result_lasso<-data.frame("Lambda" = Lasso.modFind_Lambda$lambda,"MSE" = Lasso.modFind_Lambda$cvm,"RMSE" = sqrt(Lasso.modFind_Lambda$cvm))
smallestLasso_MSE<-which.min(CV_result_lasso$MSE)
smallestLasso_RMSE<-which.min(CV_result_lasso$RMSE)
CV_result_lasso[smallestLasso_MSE,]
CV_result_lasso[smallestLasso_RMSE,]
```



## Part 5: Elastic Net

```{r find alpha and lambda with smallest RMSE}
#Set a sequence of numbers from 0 to 1, increment by 0.01 each time 
alphaseq<-seq(from = 0, to = 1, by = 0.01)
#Create empty data frame to hold alpha, lambda and RMSE of each model fitted with corresponding tuning parameter
storageElastic<-data.frame("Alpha" = rep(NA,length(alphaseq)), "Lambda" = rep(NA,length(alphaseq)),"MSE" = rep(NA, length(alphaseq)) ,"RMSE" = rep(NA,length(alphaseq)))

a=1
#Set seed for random sampling 
set.seed(1)
for(i in alphaseq){
  Elastic.modFind<-cv.glmnet(XD[,-1],vrboCleaned$price,alpha = i, lambda = seq(from = 0, to = 25,by = .5))
  storageElastic$Lambda[a]<-Elastic.modFind$lambda.min
  storageElastic$MSE[a]<-min(Elastic.modFind$cvm)
  storageElastic$RMSE[a]<-sqrt((min(Elastic.modFind$cvm)))
  storageElastic$Alpha[a]<-i
  a=a+1
}

```


```{r elastic metrics}
smallestElastic_RMSE<-which.min(storageElastic$RMSE)
storageElastic[smallestElastic_RMSE,]
```


```{r elastic final model}
elastic.modFinal<-glmnet(XD[,-1],vrboCleaned$price,alpha = 0.15,lambda = 5)

Betas_elastic<-as.numeric(coefficients(elastic.modFinal))
elastic_coef<-data.frame("Coefficients" = Betas_elastic)
rownames(elastic_coef)<-colnames(XD)

knitr::kable(elastic_coef,caption = "Table  5.1 Coefficients of Model using Elastic Net")
```



## Part 6: Comparison and Conclusion

```{r}

```


```{r}
Coefficient_Comparison<-data.frame("Ridge" = rep(NA,62), "Lasso" = rep(NA,62),"Elastic Net" = rep(NA,62))
Coefficient_Comparison$Ridge=ridge_coef$Coefficients
Coefficient_Comparison$Lasso=lasso_coef$Coefficients
Coefficient_Comparison$Elastic.Net=elastic_coef$Coefficients



knitr::kable(Coefficient_Comparison)
```



| Method      	| Tuning Parameter                	| Test MSE 	| Test RMSE 	| Features 	|
|-------------	|---------------------------------	|----------	|-----------	|----------	|
| Ridge       	| 13.1                            	| 3620.68  	| 60.17     	| 62       	|
| Lasso       	| 0.75                            	| 3643.55  	| 60.36     	| 34       	|
| Elastic Net 	| $\alpha$ = 0.15, $\lambda$ = 5  	| 3531.36  	| 59.34     	| 37       	|