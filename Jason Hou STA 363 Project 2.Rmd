---
title: "Jason Hou STA 363 Project 2"
author: "Jason Hou"
date: '2022-10-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages}
##Here we load the packages needed
library(ggplot2)
library(broom)
library(gridExtra)
library(dplyr)
library(class)
library(tidyverse)
library(gridExtra)
library(leaps)
library(corrplot)
library(RColorBrewer)
library(glmnet)
library(xtable)
```
# Abstract

## Part 1 Introduction of Data

```{r load data}
vrboData<-read.csv("VRBO.csv")
```

```{r take a look}
glimpse(vrboData)
```


## Part 2: Data Cleaning

```{r checking and cleaning}
# Find out the number of missing values in the vrbo data set
sum(is.na(vrboData))
#Remove rows with missing values 


vrboCleaned<-vrboData%>%
  na.omit(vrboData)%>%
  select(-UnitNumber)
vrboCleaned$PctRentals=as.numeric((vrboCleaned$PctRentals)*100)
```

```{r distribution}
ggplot(data=vrboCleaned,aes(x=price))+
  geom_boxplot()+
  ggtitle("Figure 2.1 Distribution of Price")
```


```{r}
knitr::kable(table(vrboData$room_type))
```

We identified that the UnitNumber is a feature that should be removed from the data set before building the model, as it has 1423 different levels, and is irrelevant to the price of the vrbo rentals.A new data set "VrboCleaned" was created for the following sections. 


```{r convert data type}
#convert character features into factor
vrboCleaned$neighborhood <-as.factor(vrboCleaned$neighborhood)
```

```{r correlation plot}
#Obtain correlation between features and save it as an object
M<-cor(vrboCleaned[,-c(4,8,9)])
#Create correlation plot
corrplot(M, method= "circle",type = "upper",title="Figure 2.2 Correlation Plot of Numeric Variables in Vrbo Data Set ",mar = c(0,0,2,0))
#M
```

We created a correlation plot for numeric features in the data set to explore if there are potential correlations present between features that allows us to determine the proper regression methods for building the model. Figure 2.1 shows that several significant correlations are presented between features, indicating shrinkage and selection techniques are appropriate to be used on building the model.








## Part 3: Ridge Regression

Here we use the ridge regression to fit the model, as it provides the effect of shrinkage. That is,shrinking the coefficients of correlated features, therefore limits the impacts from these features to the model. For this we need to find the tuning parameter that provides the best shirking effect. 

We are going to use the root mean square error(RMSE) to assess the predictive accuracy of each model that uses different values of $\lambda$(tuning parameter). The RMSE is defined as the square root of the mean squared error, which is the mean of the difference between the predicted values and observed values. The RMSE allows use to assess how well the model fits the observed data points, so that we can determine the predictive accuracy of the model. Lower RMSE indicates a higher predictive accuracy, higher RMSE, on the other hand, indicates lower predictive accuracy.


```{r creating design matrix ridge}
# Create the design matrix for ridge regression
XD <- model.matrix(price ~., data = vrboCleaned)
```



```{r cv to find the best lambda}
#Set seed for random sampling
set.seed(1)
#Run cross validation for lambda value from 1 to 50(increment by 0.05 for each run)
ridge.modFind_Lambda <- cv.glmnet(XD[,-1], vrboCleaned$price , alpha = 0 , lambda = seq(from=0, to=50,by=0.05) , standardize = TRUE)
```


```{r load a function, include=FALSE}
##Load a function for creating the plot of RMSE in response of change in Lambda
ridgePlot <- function(ridge.mod, metric, title){
  library(ggplot2)
  
  smallestLambda <- ridge.mod$lambda[which.min(ridge.mod$cvm)] 
  
  if(metric == "MSE"){
  g1 <- ggplot( data.frame(ridge.mod$lambda), aes( x = ridge.mod$lambda, y = (ridge.mod$cvm))) + geom_point() + geom_vline( xintercept = smallestLambda, col = "blue" , lty = 2 ) +  labs(caption = paste("Test MSE values for Different Tuning Parameters. Smallest MSE at lambda = ", smallestLambda), title = title, y = "Test MSE", x = "Tuning Parameter")
  
  }
  
  if(metric == "RMSE"){
  g1 <- ggplot( data.frame(ridge.mod$lambda), aes( x = ridge.mod$lambda, y = sqrt(ridge.mod$cvm))) + geom_point() + geom_vline( xintercept = smallestLambda, col = "blue" , lty = 2 ) +  labs(caption = paste("Test RMSE values for Different Tuning Parameters. Smallest RMSE at lambda = ", smallestLambda), title = title, y = "Test RMSE", x = "Tuning Parameter")

  }
  
  g1
}
```


```{r plot to determine the best lambda}
#Using the ridgePlot function(imported) to create graph of RMSE in response to Lambda
ridgePlot(ridge.modFind_Lambda, metric = "RMSE" , title ="Figure 3.1 RMSE in Response to Lambda(Ridge)" )

```

```{r ridge metrics}
#Create dataframe to hold lambda value
CV_result_ridge<-data.frame("Lambda" = ridge.modFind_Lambda$lambda,"MSE" = ridge.modFind_Lambda$cvm,"RMSE" = sqrt(ridge.modFind_Lambda$cvm))
smallestRidge_MSE<-which.min(CV_result_ridge$MSE)
smallestRidge_RMSE<-which.min(CV_result_ridge$RMSE)
#CV_result_ridge[smallestRidge_MSE,]
CV_result_ridge[smallestRidge_RMSE,]
```


We can see from Figure 3.1 that the smallest RMSE appeared when the model is using $\lambda = 13.1$ as the tuning parameter for the ridge regression. Thus, we chooses this value of $\lambda$ as the tuning parameter for our final ridge regression model.


```{r ridge}
#Use ridge regression to fit the final model with chosen tuning parameter
ridge.mod_Final <- glmnet(XD[,-1], vrboCleaned$price , alpha = 0 , lambda = 13.1 , standardize = TRUE)
#Save coefficients for the ridge final model
Betas_ridge<-as.numeric(coefficients(ridge.mod_Final))
#Create a dataframe to hold the coefficients from ridge final model
ridge_coef<-data.frame("Coefficients" = Betas_ridge)
#Add a column with feature names corresponding to each coefficient
rownames(ridge_coef)<-colnames(XD)
#Create formatted table for coefficients
knitr::kable(ridge_coef,caption = "Table 3.1 Coefficients of Model using Ridge Regression")

```





## Part 4: Lasso

```{r}
#Set seed for random sampling process within the cv.glmnet
set.seed(1)
#Fit modesls for a serie of lambda values, respectively 
Lasso.modFind_Lambda <- cv.glmnet(XD[,-1], vrboCleaned$price , alpha = 1 , lambda = seq(from=0, to=50,by=0.05) , standardize = TRUE)

#Plot the change of RMSE in reponse to lambda values
ridgePlot(Lasso.modFind_Lambda, metric = "RMSE" , title ="Figure 4.1 RMSE in Response to Lambda(Lasso)" )

```




```{r lasso metrics} 
CV_result_lasso<-data.frame("Lambda" = Lasso.modFind_Lambda$lambda,"MSE" = Lasso.modFind_Lambda$cvm,"RMSE" = sqrt(Lasso.modFind_Lambda$cvm))
smallestLasso_MSE<-which.min(CV_result_lasso$MSE)
smallestLasso_RMSE<-which.min(CV_result_lasso$RMSE)
CV_result_lasso[smallestLasso_MSE,]
CV_result_lasso[smallestLasso_RMSE,]
```
We can see from Figure 4.1 that the lowest RMSE appeared when the model is using $\lambda=0.75$ as the tuning parameter. Thus, we choose $\lambda = 0.75$ as the $\lambda$ value for the final model.

```{r lasso final}
lasso.mod_Final <- glmnet(XD[,-1], vrboCleaned$price , alpha = 1 , lambda = 0.75 , standardize = TRUE)
Betas_lasso<-as.numeric(coefficients(lasso.mod_Final))
lasso_coef<-data.frame("Coefficients" = Betas_lasso)

rownames(lasso_coef)<-colnames(XD)
knitr::kable(lasso_coef,caption = "Table 4.1 Coefficients of Model using Lasso Regression")

```





## Part 5: Elastic Net

```{r find alpha and lambda with smallest RMSE}
#Set a sequence of numbers from 0 to 1, increment by 0.01 each time 
alphaseq<-seq(from = 0, to = 1, by = 0.01)
#Create empty data frame to hold alpha, lambda and RMSE of each model fitted with corresponding tuning parameter
storageElastic<-data.frame("Alpha" = rep(NA,length(alphaseq)), "Lambda" = rep(NA,length(alphaseq)),"MSE" = rep(NA, length(alphaseq)) ,"RMSE" = rep(NA,length(alphaseq)))

a=1
#Set seed for random sampling 
set.seed(1)
for(i in alphaseq){
  Elastic.modFind<-cv.glmnet(XD[,-1],vrboCleaned$price,alpha = i, lambda = seq(from = 0, to = 25,by = .5))
  storageElastic$Lambda[a]<-Elastic.modFind$lambda.min
  storageElastic$MSE[a]<-min(Elastic.modFind$cvm)
  storageElastic$RMSE[a]<-sqrt((min(Elastic.modFind$cvm)))
  storageElastic$Alpha[a]<-i
  a=a+1
}

```


```{r elastic metrics}
smallestElastic_RMSE<-which.min(storageElastic$RMSE)
storageElastic[smallestElastic_RMSE,]
```


```{r elastic final model}
elastic.modFinal<-glmnet(XD[,-1],vrboCleaned$price,alpha = 0.15,lambda = 5)

Betas_elastic<-as.numeric(coefficients(elastic.modFinal))
elastic_coef<-data.frame("Coefficients" = Betas_elastic)
rownames(elastic_coef)<-colnames(XD)

knitr::kable(elastic_coef,caption = "Table  5.1 Coefficients of Model using Elastic Net")
```



## Part 6: Comparison and Conclusion




```{r coefficient data frame}
#Create a data frama to hold all the coefficeints from each regression model
Coefficient_Comparison<-data.frame("Ridge" = rep(NA,62), "Lasso" = rep(NA,62),"Elastic Net" = rep(NA,62))
#Assign coefficeints to corresponding colums in the data set
Coefficient_Comparison$Ridge=ridge_coef$Coefficients
Coefficient_Comparison$Lasso=lasso_coef$Coefficients
Coefficient_Comparison$Elastic.Net=elastic_coef$Coefficients



knitr::kable(Coefficient_Comparison)
```



| Method      	| Tuning Parameter                	| Test MSE 	| Test RMSE 	| Features 	|
|-------------	|---------------------------------	|----------	|-----------	|----------	|
| Ridge       	| 13.1                            	| 3620.68  	| 60.17     	| 62       	|
| Lasso       	| 0.75                            	| 3643.55  	| 60.36     	| 34       	|
| Elastic Net 	| $\alpha$ = 0.15, $\lambda$ = 5  	| 3531.36  	| 59.34     	| 37       	|